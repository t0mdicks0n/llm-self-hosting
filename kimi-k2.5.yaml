name: kimi-k2-5

resources:
  cloud: gcp
  region: us-central1
  accelerators: {H100:8, A100-80GB:8}
  use_spot: true
  disk_size: 1000
  image_id: projects/deeplearning-platform-release/global/images/pytorch-2-7-cu128-ubuntu-2204-nvidia-570-v20260129

setup: |
  set -e

  # Install vLLM with CUDA 12.8 wheels to match the VM image (driver 570).
  # Default pip install pulls cu129 which fails on driver 570.
  pip install uv
  uv pip install --system vllm --torch-backend=cu128
  pip install flash-attn --no-build-isolation

  # Download model weights to local disk (fast), not GCS FUSE mount (slow)
  # Check GCS cache first for subsequent launches
  LOCAL_MODEL=$HOME/Kimi-K2.5
  BUCKET=kimi-k2-5-weights

  if gsutil -q stat gs://$BUCKET/.download_complete 2>/dev/null; then
    echo "Model weights cached in GCS. Downloading to local disk via gsutil..."
    mkdir -p $LOCAL_MODEL
    gsutil -m cp -r gs://$BUCKET/Kimi-K2.5/* $LOCAL_MODEL/
    echo "Copy from GCS complete."
  else
    echo "Downloading Kimi K2.5 model weights to local disk (~595GB)..."
    python -c "
  from huggingface_hub import snapshot_download
  snapshot_download('moonshotai/Kimi-K2.5', local_dir='$LOCAL_MODEL')
  "
    echo "Download complete. Uploading to GCS cache for future launches..."
    gsutil -m cp -r $LOCAL_MODEL/* gs://$BUCKET/Kimi-K2.5/ && gsutil cp /dev/null gs://$BUCKET/.download_complete &
    echo "GCS sync started in background."
  fi

run: |
  echo "Starting vLLM server for Kimi K2.5..."
  echo "GPUs available: $SKYPILOT_NUM_GPUS_PER_NODE"
  nvidia-smi

  # GCP a3-highgpu-8g VM images ship with GIB (GPU Interconnect Bridge) libs
  # in LD_LIBRARY_PATH. GIB is for multi-node InfiniBand communication.
  # On single-node, the GIB NCCL shim plugin interferes with P2P/NVSwitch init.
  # Remove it so NCCL auto-detects P2P over NVLink/NVSwitch natively.
  export LD_LIBRARY_PATH=$(echo "$LD_LIBRARY_PATH" | tr ':' '\n' | grep -v gib | paste -sd ':')

  LOCAL_MODEL=$HOME/Kimi-K2.5

  vllm serve $LOCAL_MODEL \
    -tp $SKYPILOT_NUM_GPUS_PER_NODE \
    --served-model-name Kimi-K2.5 \
    --mm-encoder-tp-mode data \
    --tool-call-parser kimi_k2 \
    --reasoning-parser kimi_k2 \
    --trust-remote-code \
    --host 0.0.0.0 \
    --port 8000
